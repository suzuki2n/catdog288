{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.client import device_lib\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# シード値を固定\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# 🐱🐶 画像分類モデルの学習\n",
    "print(Sequential)\n",
    "# GPUがあるか確認して、あればメモリ制限\n",
    "print(device_lib.list_local_devices())\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"✅ GPUが見つかりました\")\n",
    "    try:\n",
    "        # 動的メモリ割り当てを有効化\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"✅ GPUメモリの動的割り当てを設定しました\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"❌ メモリ設定に失敗しました：\", e)\n",
    "        \n",
    "# 🟡 ここに設定をまとめ\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 12 # 1回の学習で使う画像の枚数\n",
    "EPOCHS = 50 # 「全部の画像 × 3周」を学習\n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)  # 画像の縦・横・チャンネル数\n",
    "\n",
    "train_dir = 'dataset/train'\n",
    "val_dir = 'dataset/validation'\n",
    "\n",
    "# 📁 データの前処理\n",
    "# 訓練用ジェネレーター（オーグメンテーション付き）（回転やズームなど）をかけながら、ランダムに画像を取り出して学習させる\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # rescale=1./255,\n",
    "    preprocessing_function=preprocess_input,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    rotation_range=15,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# ディレクトリから画像を読み込む\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # 訓練データのパス\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True,\n",
    "    seed=seed_value\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,\n",
    "    seed=seed_value # 乱数シードを固定\n",
    "    # shuffle=False, # シャッフルしない場合は、Trueにするとエラーになるので注意\n",
    ")\n",
    "\n",
    "print(f\"✅ 訓練データ数: {train_generator.samples}\")\n",
    "print(f\"✅ 検証データ数: {val_generator.samples}\")\n",
    "print(f\"✅ クラス: {train_generator.class_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 モデル構築\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ResNet50のベースモデル（ImageNet学習済み）を使う\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # 転移学習のため、ベースモデルの重みは固定する\n",
    "   \n",
    "# カスタム分類層を上に乗せる（改善版）\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)  # バッチ正規化を追加\n",
    "\n",
    "# より深いネットワーク構造\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)  # バッチ正規化を追加\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)  # バッチ正規化を追加\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)  # バッチ正規化を追加\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# モデル定義\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# ---------- 第1段階 (全凍結) ----------\n",
    "model.compile(optimizer=Adam(1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks_stage1 = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10,\n",
    "                  restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                      patience=5, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint('best_stage1.h5', monitor='val_accuracy',\n",
    "                    save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "history_1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_stage1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------- 第2段階 (Fine-tuning) ----------\n",
    "# 末尾30層だけ学習させる\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(1e-5),   # lr を必ず小さく\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks_stage2 = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=15,\n",
    "                  restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                      patience=7, min_lr=1e-8, verbose=1),\n",
    "    ModelCheckpoint('best_stage2_finetuned.h5', monitor='val_accuracy',\n",
    "                    save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "history_2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_stage2,\n",
    "    verbose=1,\n",
    "    initial_epoch=len(history_1.history['loss'])  # 続きから\n",
    ")\n",
    "\n",
    "# ---------- 最終モデル保存 ----------\n",
    "model.save('cat_dog_final.h5')\n",
    "# モデルの要約を表示\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 モデル評価\n",
    "# \"validation\" フォルダの中にある画像を使って、モデルの評価を行う\n",
    "# ➊ 検証用ジェネレーターを作成（shuffle=False でファイル順に対応）\n",
    "# 画像に手を加えずに、そのまま順番に取り出して評価する\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=1,\n",
    "    class_mode='binary',\n",
    "    shuffle=False,                  # 検証はシャッフルしない\n",
    "    seed=seed_value \n",
    ")\n",
    "\n",
    "# ➊ モデルの評価（検証データ全体での最終的な loss と accuracy を取得）\n",
    "loss_eval, acc_eval = model.evaluate(val_gen, verbose=1)\n",
    "print(f\"▶ 最終検証損失: {loss_eval:.4f}\")\n",
    "print(f\"▶ 最終検証精度: {acc_eval:.4f}\")\n",
    "print(\"▶ モデルの評価が完了しました\")\n",
    "\n",
    "from datetime import datetime\n",
    "fname = f\"cat_dog_{datetime.now():%Y%m%d_%H%M}_acc{acc_eval:.2f}.h5\"\n",
    "model.save(fname)\n",
    "print(f\"モデルを {fname} に保存しました🐾\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 学習結果の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "# フォント設定（例：Windows + メイリオ）\n",
    "plt.rcParams['font.family'] = 'Meiryo'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def plot_training_history(history_1, history_2=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # 精度のプロット\n",
    "    epochs_1 = range(1, len(history_1.history['accuracy']) + 1)\n",
    "    ax1.plot(epochs_1, history_1.history['accuracy'], 'bo-', label='Training acc (Stage 1)')\n",
    "    ax1.plot(epochs_1, history_1.history['val_accuracy'], 'ro-', label='Validation acc (Stage 1)')\n",
    "    \n",
    "    if history_2:\n",
    "        epochs_2 = range(len(epochs_1) + 1, len(epochs_1) + len(history_2.history['accuracy']) + 1)\n",
    "        ax1.plot(epochs_2, history_2.history['accuracy'], 'bs-', label='Training acc (Stage 2)')\n",
    "        ax1.plot(epochs_2, history_2.history['val_accuracy'], 'rs-', label='Validation acc (Stage 2)')\n",
    "    \n",
    "    ax1.set_title('Model accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 損失のプロット\n",
    "    ax2.plot(epochs_1, history_1.history['loss'], 'bo-', label='Training loss (Stage 1)')\n",
    "    ax2.plot(epochs_1, history_1.history['val_loss'], 'ro-', label='Validation loss (Stage 1)')\n",
    "    \n",
    "    if history_2:\n",
    "        ax2.plot(epochs_2, history_2.history['loss'], 'bs-', label='Training loss (Stage 2)')\n",
    "        ax2.plot(epochs_2, history_2.history['val_loss'], 'rs-', label='Validation loss (Stage 2)')\n",
    "    \n",
    "    ax2.set_title('Model loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# グラフを表示\n",
    "plot_training_history(history_1, history_2)\n",
    "\n",
    "# 6. 予測結果を CSV に出力\n",
    "import pandas as pd\n",
    "preds       = model.predict(val_gen, steps=len(val_gen), verbose=1)\n",
    "pred_labels = (preds >= 0.5).astype(int).flatten()\n",
    "true_labels = val_gen.classes\n",
    "filenames   = val_gen.filenames\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'true':     true_labels,\n",
    "    'pred':     pred_labels\n",
    "})\n",
    "df.attrs['param'] = {'epoch':EPOCHS, 'batch':BATCH_SIZE,\n",
    "                     'lr':1e-4, 'aug':'std'}\n",
    "df['correct'] = df['true'] == df['pred']\n",
    "df.to_csv('classification_results.csv', index=False)\n",
    "print(\"▶ classification_results.csv に書き出しました\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
